{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b549811",
   "metadata": {},
   "source": [
    "<center><h1>网络爬虫</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe070d6",
   "metadata": {},
   "source": [
    "## 网络爬虫是一种按照一定的规则自动地获取网页源码、从源码中提取相关的信息、数据存储。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b8b309",
   "metadata": {},
   "source": [
    "网络爬虫是一个自动提取网页的程序，它为搜索引擎从万维网上下载网页，是搜索引擎的重要组成部分。如图所示，爬虫从一个或若干个初始网页的URL开始，获得初始网页上的URL，在抓取网页的过程中，不断从当前页面上抽取新的URL放入队列，直到满足系统的一定停止条件。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc38cfc",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_2_11.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167f916",
   "metadata": {},
   "source": [
    "# 2.0网页基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b46049",
   "metadata": {},
   "source": [
    "## 2.0.1 超文本和HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bbcd1e0",
   "metadata": {},
   "source": [
    "超文本（Hypertext）是指使用超链接的方法，把文字和图片信息相互联结，形成具有相关信息的体系。超文本的格式有很多，目前最常使用的是超文本标记语言HTML（Hyper Text Markup Language），我们平时在浏览器里面看到的网页就是由HTML解析而成的。下面是网页文件web_demo.html的HTML源代码："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a038d6cf",
   "metadata": {},
   "source": [
    "```html\n",
    "<html><head><title>搜索指数</title></head>\n",
    "<body>\n",
    "<table>\n",
    "<tr><td>排名</td><td>关键词</td><td>搜索指数</td></tr>\n",
    "<tr><td>1</td><td>大数据</td><td>187767</td></tr>\n",
    "<tr><td>2</td><td>云计算</td><td>178856</td></tr>\n",
    "<tr><td>3</td><td>物联网</td><td>122376</td></tr>\n",
    "</table>\n",
    "</body>\n",
    "</html>\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b2f2cf",
   "metadata": {},
   "source": [
    "## 2.0.2 HTTP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5395c1e",
   "metadata": {},
   "source": [
    "HTTP是由万维网协会（World Wide Web Consortium）和 Internet 工作小组IETF（Internet Engineering Task Force）共同制定的规范。HTTP的全称是“Hyper Text Transfer Protocol”，中文名叫做“超文本传输协议”。HTTP协议是用于从网络传输超文本数据到本地浏览器的传送协议，它能保证高效而准确地传送超文本内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3520738e",
   "metadata": {},
   "source": [
    "HTTP是基于“客户端/服务器”架构进行通信的，HTTP的服务器端实现程序有httpd、nginx等，客户端的实现程序主要是Web浏览器，例如Firefox、Internet Explorer、Google Chrome、Safari、Opera等。Web浏览器和Web服务器之间可以通过HTTP进行通信。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb028228",
   "metadata": {},
   "source": [
    "一个典型的HTTP请求过程如下（如图所示）： （1）用户在浏览器中输入网址，比如http://www.cucn.edu.cn， 浏览器向网页服务器发起请求； （2）网页服务器接收用户访问请求，处理请求，产生响应（即把处理结果以HTML形式返回给浏览器）； （3）浏览器接收来自网页服务器的HTML内容，进行渲染以后展示给用户。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ac3cf1",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_2_13.png\" width=\"600\" height=\"600\" >   \n",
    "<center>一个典型的HTTP请求过程</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b178e142",
   "metadata": {},
   "source": [
    "### urllib库\n",
    "### Requests\n",
    "### BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26c6ca0",
   "metadata": {},
   "source": [
    "# 2.1 urllib库"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb20af",
   "metadata": {},
   "source": [
    "```\n",
    "urllib.request.urlopen(url,data=None,[timeout,]*,\n",
    "                     cafile=None,capath=None,\n",
    "                     cadefault=False, context=None)\n",
    "```\n",
    "●  url：需要打开的网址。\n",
    "\n",
    "●  data：Post提交的数据。\n",
    "\n",
    "●  timeout：设置网站的访问超时时间。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798a082f",
   "metadata": {},
   "source": [
    "urllib是Python自带模块，该模块提供了一个urlopen()方法，通过该方法指定URL发送HTTP请求来获取数据。urllib提供了多个子模块，具体的模块名称与功能如表所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d4b91d",
   "metadata": {},
   "source": [
    "|  模块名称   | 功能 |\n",
    "|  :----  | :----  |\n",
    "| urllib.request  | 该模块定义了打开URL（主要是HTTP）的方法和类，如身份验证、重定向和cookie等 |\n",
    "| urllib.error | 该模块中主要包含异常类，基本的异常类是URLError |\n",
    "| urllib.parse  | 该模块定义的功能分为两大类：URL解析和URL引用 |\n",
    "| urllib.robotparser  | 该模块用于解析robots.txt文件 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71565f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是通过urllib.request模块实现发送GET请求获取网页内容的实例：\n",
    "import urllib.request\n",
    "response=urllib.request.urlopen(\"http://www.baidu.com\")\n",
    "html=response.read()\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f60c3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面是通过urllib.request模块实现发送POST请求获取网页内容的实例：\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "# 1.指定url\n",
    "url = 'https://fanyi.baidu.com/sug'\n",
    "# 2.发起POST请求之前，要处理POST请求携带的参数\n",
    "# 2.1 将POST请求封装到字典\n",
    "data = {'kw':'苹果',}\n",
    "# 2.2 使用parse模块中的urlencode(返回值类型是字符串类型)进行编码处理\n",
    "data = urllib.parse.urlencode(data)\n",
    "# 将步骤2.2的编码结果转换成byte类型\n",
    "data = data.encode()\n",
    "# 3.发起POST请求:urlopen函数的data参数表示的就是经过处理之后的POST请求携带的参数\n",
    "response = urllib.request.urlopen(url=url,data=data)\n",
    "data = response.read()\n",
    "# print(data)\n",
    "s = data.decode('utf-8')\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3399c557",
   "metadata": {},
   "source": [
    "把上面print(data)执行的结果，拿到JSON在线格式校验网站 http://www.bejson.com 进行处理，使用“Unicode转中文”功能可以得到如下结果"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f54b1bc",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c2d6c7",
   "metadata": {},
   "source": [
    "# 2.1 Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e943ef",
   "metadata": {},
   "source": [
    "requests库是一个非常好用的HTTP请求库，可用于网络请求和网络爬虫等。在使用requests之前，需要打开一个cmd窗口使用如下命令进行安装： pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c54292b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 以GET请求方式为例，打印多种请求信息的代码如下：\n",
    "import requests\n",
    "response = requests.get('http://www.baidu.com')  #对需要爬取的网页发送请求\n",
    "print('状态码:',response.status_code)  #打印状态码\n",
    "print('-'*20)\n",
    "print('url:',response.url)  #打印请求url\n",
    "print('-'*20)\n",
    "print('header:',response.headers)  #打印头部信息\n",
    "print('-'*20)\n",
    "print('cookie:',response.cookies)  #打印cookie信息\n",
    "print('-'*20)\n",
    "print('text:',response.text)  #以文本形式打印网页源码\n",
    "print('-'*20)\n",
    "print('content:',response.content)  #以字节流形式打印网页源码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53610c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以POST请求方式发送HTTP网页请求的示例代码如下：\n",
    "import requests\n",
    "#导入模块\n",
    "import requests\n",
    "#表单参数\n",
    "data = {'kw':'苹果',}\n",
    "#对需要爬取的网页发送请求\n",
    "response = requests.post('https://fanyi.baidu.com/sug',data=data)\n",
    "#以字节流形式打印网页源码\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864a5c4f",
   "metadata": {},
   "source": [
    "## 定制requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e76e6",
   "metadata": {},
   "source": [
    "* 2.1.1 传递URL参数\n",
    "* 2.1.2 定制请求头\n",
    "* 2.1.3 网络超时"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1cf1fe",
   "metadata": {},
   "source": [
    "## 2.1.1传递URL参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d019d03",
   "metadata": {},
   "source": [
    "为了请求特定的数据，我们需要在URL（Uniform Resource Locator）的查询字符串中加入一些特定数据。这些数据一般会跟在一个问号后面，并且以键值对的形式放在URL中。在requests中，我们可以直接把这些参数保存在字典中，用params构建到URL中。具体实例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d96b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "base_url = 'http://httpbin.org'\n",
    "param_data = {'user':'xmu','password':'123456'}\n",
    "response = requests.get(base_url+'/get',params=param_data)\n",
    "print(response.url)\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c9337",
   "metadata": {},
   "source": [
    "## 2.1.2 定制请求头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3442632",
   "metadata": {},
   "source": [
    "  在爬取网页的时候，输出的信息中有时候会出现“抱歉，无法访问”等字眼，这就是禁止爬取，需要通过定制请求头Headers来解决这个问题。定制Headers是解决requests请求被拒绝的方法之一，相当于我们进入这个网页服务器，假装自己本身在爬取数据。请求头Headers提供了关于请求、响应或其他发送实体的消息，如果没有定制请求头或请求的请求头和实际网页不一致，就可能无法返回正确结果。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d92c3ee",
   "metadata": {},
   "source": [
    "获取一个网页的Headers的方法如下：使用360、火狐或谷歌浏览器打开一个网址（比如http://httpbin.org/ , 在网页上单击鼠标右键，在弹出的菜单中选择“查看元素”，然后刷新网页，再按照如下图所示的步骤，先点击“Network”选项卡，再点击“Doc”，接下来点击“Name”下方的网址，就会出现类似如下的Headers信息： User-Agent:Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb4ed6a",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cefd02",
   "metadata": {},
   "source": [
    "Headers中有很多内容，主要常用的就是“User-Agent”和“Host”，它们是以键对的形式呈现的，如果把“User-Agent”以字典键值对形式作为Headers的内容，往往就可以顺利爬取网页内容。 下面是添加了Headers信息的网页请求过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7bff83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url='http://httpbin.org'\n",
    "# 创建头部信息\n",
    "headers={'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/46.0.2490.86 Safari/537.36'}\n",
    "response = requests.get(url,headers=headers)\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d070bb7",
   "metadata": {},
   "source": [
    "## 2.1.3 网络超时"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ace68f",
   "metadata": {},
   "source": [
    "网络请求不可避免会遇上请求超时的情况，这个时候，网络数据采集的程序会一直运行等待进程，造成网络数据采集程序不能很好地顺利执行。因此，可以为requests的timeout参数设定等待秒数，如果服务器在指定时间内没有应答就返回异常。具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557be3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.exceptions import ReadTimeout,ConnectTimeout\n",
    "try:\n",
    "   response = requests.get(\"http://www.baidu.com\", timeout=0.5)\n",
    "   print(response.status_code)\n",
    "except ReadTimeout or ConnectTimeout:\n",
    "   print('Timeout')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cdc34f",
   "metadata": {},
   "source": [
    "# 2.2 BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e53236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "344985c7",
   "metadata": {},
   "source": [
    "使用BeautifulSoup库前要安装该库：pip install beautifulsoup4。\n",
    "\n",
    "创建BeautifulSoup库的对象，需从bs4库导入：from bs4 import BeautifulSoup。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2457539",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import urllib\n",
    "html = urllib.request.urlopen(r'https://www.baidu.com')\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2a86b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "html.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from bs4 import BeautifulSoup\n",
    "html = urllib.request.urlopen(r'http://www.baidu.com/')\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "soup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0aedf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.prettify() #有时为了代码的层次感更清晰，也可以使用print(soup.prettify())显示网页源码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320f3401",
   "metadata": {},
   "source": [
    "在写CSS时，标签名不需要加任何修饰，类名前加下角点“.”，id名前加井号“#”。  \n",
    "在这里我们也可以利用类似的方法来筛选元素，采用的方法是soup.select()，返回类型是列表list。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003c28e",
   "metadata": {},
   "source": [
    "### (1)通过标签名查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8d8f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4313859d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761ed6f",
   "metadata": {},
   "source": [
    "### (2)通过类名查找。类名前加“.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e44663",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('.c-tips-container'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e090b3",
   "metadata": {},
   "source": [
    "### (3)通过id名查找。id前加“#”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267cff01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('#c-tips-container'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35366c27",
   "metadata": {},
   "source": [
    "### (4)组合查找"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4247c99d",
   "metadata": {},
   "source": [
    "组合查找时，标签名与类名、id名进行单独查找方法一样，组合时只需用空格隔开。  \n",
    "例如，查找div标签中，id等于s_qrcode_nologin的内容，二者需要用空格分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4fc639",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup.select('div #s_qrcode_nologin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec1dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 直接子标签查找，标签之间加“>”。\n",
    "print(soup.select(\"div > img\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c042cc08",
   "metadata": {},
   "source": [
    "### (5)属性查找"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0580d395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找时还可以加入属性元素，属性需要用中括号括起来，注意属性和标签属于同一结点，所以中间不能加空格，否则会无法匹配\n",
    "print(soup.select('a[href=\"http://www.baidu.com/more/\"]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e03eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 同样，属性仍然可以与上述查找方式组合，不在同一结点的用空格隔开，同一结点的不加空格。\n",
    "print(soup.select('div a[href=\"http://www.baidu.com/more/\"]'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda7b00e",
   "metadata": {},
   "source": [
    "### (6)通过findAll和find_all()函数查找"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f42d2e8",
   "metadata": {},
   "source": [
    "> findAll(name=None, attrs={}, recursive=True, \n",
    "              text=None, limit=None, **kwargs) \n",
    "              \n",
    "返回一个列表，其中最重要的参数是name和keywords。   \n",
    "参数name匹配tags的名字，获得相应的结果集。有几种方法匹配name，最简单的用法是仅仅给定一个tag的name值。   \n",
    "① 搜索网页源码中所有b标签：soup.findAll('b')。  \n",
    "② 可以传一个正则表达式，下面的代码寻找所有以b开头的标签。\n",
    "> import re\n",
    "tagsStartingWithB = soup.findAll(re.compile('^b'))\n",
    "print(tagsStartingWithB)\n",
    "\n",
    "③ 可以传一个list或dictionary。查找所有的title和p标签，获得结果一样，但方法2更快一些\n",
    "\n",
    "> 方法1：  soup.findAll(['title', 'p'])  \n",
    " 方法2：  soup.findAll({'title' : True, 'p' : True})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbe53d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "tagsStartingWithB = soup.findAll(re.compile('^b')) \n",
    "print(tagsStartingWithB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a837222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll(['title', 'p'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a76936",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.findAll({'title' : True, 'p' : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0497becc",
   "metadata": {},
   "source": [
    "# 2.3 SCrapy框架爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9659da7b",
   "metadata": {},
   "source": [
    "## 2.3.1 Scrapy爬虫概述"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac362e",
   "metadata": {},
   "source": [
    "Scrapy是一套基于Twisted的异步处理框架，是纯Python实现的爬虫框架，用户只需要定制开发几个模块就可以轻松地实现一个爬虫，用来抓取网页内容或者各种图片。Scrapy运行于Linux/Windows/MacOS等多种环境，具有速度快、扩展性强、使用简便等特点。即便是新手，也能迅速学会使用Scrapy编写所需要的爬虫程序。Scrapy可以在本地运行，也能部署到云端实现真正的生产级数据采集系统。Scrapy用途广泛，可以用于数据挖掘、监测和自动化测试。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e83bc21",
   "metadata": {},
   "source": [
    "### 1．Scrapy体系架构\n",
    "![avatar](image/Chapter2_2_7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e594c30",
   "metadata": {},
   "source": [
    "### 2．Scrapy工作流\n",
    "Scrapy工作流也叫作“运行流程”或叫作“数据处理流程”，整个数据处理流程由Scrapy引擎进行控制，其主要的运行步骤如下：  \n",
    "①Scrapy引擎从调度器中取出一个链接（URL）用于接下来的抓取；  \n",
    "②Scrapy引擎把URL封装成一个请求并传给下载器；  \n",
    "③下载器把资源下载下来，并封装成应答包；  \n",
    "④爬虫解析应答包；  \n",
    "⑤如果解析出的是项目，则交给项目管道进行进一步的处理；  \n",
    "⑥如果解析出的是链接（URL），则把URL交给调度器等待抓取。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc581c15",
   "metadata": {},
   "source": [
    "## 2.3.2 XPath语言"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f687383",
   "metadata": {},
   "source": [
    "XPath（XML Path Language）是一门在XML和HTML文档中查找信息的语言，可用来在XML和HTML文档中对元素和属性进行遍历。简单来说，网页数据是以超文本的形式来呈现的，想要获取里面的数据，就要按照一定的规则来进行数据的处理，这种规则就叫做XPath。XPath提供了超过100个内建函数，几乎所有要定位的节点都可以用XPath来定位，在做网络爬虫时可以使用XPath提取所需的信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d150c683",
   "metadata": {},
   "source": [
    "### 1．基本术语\n",
    "XML文档通常可以被看作一棵节点树。在XML中，有元素、属性、文本、命名空间、处理指令、注释以及文档节点等七种类型的节点，其中，元素节点是最常用的节点。下面是一个HTML文档中的代码：\n",
    "```\n",
    "<html>\n",
    "    <head><title>BigData Software</title></head>\n",
    "    <p class=\"title\"><b>BigData Software</b></p>\n",
    "    <p class=\"bigdata\">There are three famous bigdata software;and their names are\n",
    "        <a href=\"http://example.com/hadoop\" class=\"hadoop\" id=\"link1\">Hadoop</a>,\n",
    "        <a href=\"http://example.com/spark\" class=\"spark\" id=\"link2\">Spark</a>and\n",
    "        <a href=\"http://example.com/flink\" class=\"flink\" id=\"link3\"><!--Flink--></a>;\n",
    "        and they are widely used in real application.</p>\n",
    "    <p class=\"bigdata\">...</p>\n",
    "</html>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc86db",
   "metadata": {},
   "source": [
    "上面的HTML文档中，<html>是文档节点，<title>BigData Software</title>是元素节点，class=\"title\"是属性节点。节点之间存在下面几种关系：  \n",
    "（1）父节点：每个元素和属性都有一个父节点。例如，html节点是head节点和p节点的父节点；head节点是title节点的父节点；第二个p节点是中间三个a节点的父节点。  \n",
    "（2）子节点：每一个元素节点的下一个直接节点是该元素节点的子节点。每个元素节点可以有零个、一个或多个子节点。例如，title节点是head节点的子节点。  \n",
    "（3）兄弟节点：拥有相同父节点的节点，就是兄弟节点。例如，第二个p节点中的三个a节点就是兄弟节点；head节点和中间三个p节点就是兄弟节点；title节点和a节点就不是兄弟节点，因为不是同一个父节点。  \n",
    "（4）祖先节点：节点的父节点以及父节点的父节点等，称作“祖先节点”。例如，html节点和head节点是title节点的祖先节点。  \n",
    "（5）后代节点：节点的子节点以及子节点的子节点等，称作“后代节点”。例如，html节点的后代节点有head、title、b、p以及a节点。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aca4e30",
   "metadata": {},
   "source": [
    "### 2．基本语法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fb069f",
   "metadata": {},
   "source": [
    "XML/HTML文档是由标签构成的，所有的标签都有很强的层级关系。基于这种层级关系，XPath语法能够准确定位我们所需要的信息。XPath使用路径表达式来选取XML/HTML文档中的节点，这个路径表达式和普通计算机文件系统中见到的路径表达式非常相似。在XPath语法中，我们直接使用路径来选取，再加上适当的谓语或函数进行指定，就可以准确定位到指定的节点。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aac8e0",
   "metadata": {},
   "source": [
    "#### （1）节点选取\n",
    "XPath选取节点时，是沿着路径到达目标，下表列出了常用的表达式。\n",
    "\n",
    "![avatar](image/Chapter2_2_8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffee57c8",
   "metadata": {},
   "source": [
    "“/”可以理解为绝对路径，需要从根节点开始；“./”则是相对路径，可以从当前节点开始；“../”则是先返回上一节点，从上一节点开始。这与普通计算机的文件系统类似。下面给出测试这些表达式的简单实例，这里需要用到lxml中的etree库，在使用之前需要执行如下命令安装lxml库：\n",
    "> pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f4db83f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Element body at 0x16e3ec6ac00>]\n"
     ]
    }
   ],
   "source": [
    "# 下面是实例代码：\n",
    "html_text = \"\"\"\n",
    "<html>\n",
    "  <body>\n",
    "    <head><title>BigData Software</title></head>\n",
    "    <p class=\"title\"><b>BigData Software</b></p>\n",
    "    <p class=\"bigdata\">There are three famous bigdata software;and their names are\n",
    "      <a href=\"http://example.com/hadoop\" class=\"bigdata Hadoop\" id=\"link1\">Hadoop</a>,\n",
    "      <a href=\"http://example.com/spark\" class=\"bigdata Spark\" id=\"link2\">Spark</a>and\n",
    "      <a href=\"http://example.com/flink\" class=\"bigdata Flink\" id=\"link3\"><!--Flink--></a>;\n",
    "        and they are widely used in real application.</p>\n",
    "    <p class=\"bigdata\">others</p>\n",
    "    <p>……</p>\n",
    "  </body>\n",
    "</html>\n",
    "\"\"\"\n",
    "from lxml import etree\n",
    "html = etree.HTML(html_text)\n",
    "html_data = html.xpath('body')\n",
    "print(html_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77153e53",
   "metadata": {},
   "source": [
    "可以看出，html.xpath('body')的输出结果不是像HTML里面那样显示的标签，  \n",
    "其实这就是我们所要的元素，只不过我们还需要再进行一步操作，也就是使用etree中的.tostring()方法将其进行转换。  \n",
    "此外，html.xpath('body')的输出结果是一个列表，因此，我们可以使用for循环来遍历列表，具体代码如下：  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b2b7f",
   "metadata": {},
   "source": [
    "“//”表示全局搜索，比如，“//p”可以将所有的 <p> 标签搜索出来。\n",
    "“/”表示在某标签下进行搜索，只能搜索子节点，不能搜索子节点的子节点。\n",
    "简单来说，“//”可以进行跳级搜索，“/”只能在本级上进行搜索，不能跳跃。下面是具体实例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76c6c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<a href=\"http://example.com/hadoop\" class=\"bigdata Hadoop\" id=\"link1\">Hadoop</a>,\\n      '\n",
      "b'<a href=\"http://example.com/spark\" class=\"bigdata Spark\" id=\"link2\">Spark</a>and\\n      '\n",
      "b'<a href=\"http://example.com/flink\" class=\"bigdata Flink\" id=\"link3\"><!--Flink--></a>;\\n        and they are widely used in real application.'\n",
      "b'<a href=\"http://example.com/hadoop\" class=\"bigdata Hadoop\" id=\"link1\">Hadoop</a>,\\n      '\n",
      "b'<a href=\"http://example.com/spark\" class=\"bigdata Spark\" id=\"link2\">Spark</a>and\\n      '\n",
      "b'<a href=\"http://example.com/flink\" class=\"bigdata Flink\" id=\"link3\"><!--Flink--></a>;\\n        and they are widely used in real application.'\n"
     ]
    }
   ],
   "source": [
    "# （1）逐级搜索\n",
    "html_data = html.xpath('/html/body/p/a')\n",
    "for element in html_data:\n",
    "  print(etree.tostring(element))\n",
    "# （2）跳级搜索\n",
    "html_data = html.xpath('//a')\n",
    "for element in html_data:\n",
    "  print(etree.tostring(element))\n",
    "\n",
    "\n",
    "\n",
    "#上面两段代码的执行结果相同，具体如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d001dfc",
   "metadata": {},
   "source": [
    "可以在方括号内添加“@”，将标签属性填进去，这样就可以准确地将含有该标签属性的部分提取出来，示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbcdbc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<a href=\"http://example.com/spark\" class=\"bigdata Spark\" id=\"link2\">Spark</a>and\\n      '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "html_data = html.xpath('//p/a[@class=\"bigdata Spark\"]')\n",
    "for element in html_data:\n",
    "    print(etree.tostring(element))\n",
    "\n",
    "# 上面代码的执行结果如下："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0febc5d3",
   "metadata": {},
   "source": [
    "### （2）谓语\n",
    "直接使用前面介绍的方法可以定位到多数我们需要的节点，但是有时候我们需要查找某个特定的节点或者包含某个指定值的节点，就要用到谓语。谓语是被嵌在方括号中的。下表列出了一些带有谓语的路径表达式及其描述的内容!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ff3a1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<p class=\"bigdata\">There are three famous bigdata software;and their names are\\n      <a href=\"http://example.com/hadoop\" class=\"bigdata Hadoop\" id=\"link1\">Hadoop</a>,\\n      <a href=\"http://example.com/spark\" class=\"bigdata Spark\" id=\"link2\">Spark</a>and\\n      <a href=\"http://example.com/flink\" class=\"bigdata Flink\" id=\"link3\"><!--Flink--></a>;\\n        and they are widely used in real application.</p>\\n    '\n",
      "b'<p class=\"bigdata\">others</p>\\n    '\n"
     ]
    }
   ],
   "source": [
    "html_data = html.xpath('//body/p[@class=\"bigdata\"]')\n",
    "for element in html_data:\n",
    " print(etree.tostring(element))\n",
    "\n",
    "# 上面代码执行结果如下：\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f6f6b",
   "metadata": {},
   "source": [
    "#### （3）函数\n",
    "XPath中提供超过100个内建函数用于字符串值、数值、日期和时间比较序列处理等操作，极大地方便了我们定位获取所需要的信息。表3-5列出了几个常用的函数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3449934f",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_2_10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c54d27c",
   "metadata": {},
   "source": [
    "> pip install scrapy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b322cba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop', 'Spark']\n"
     ]
    }
   ],
   "source": [
    "html = etree.HTML(html_text)\n",
    "html_data = html.xpath('//a[contains(@class, \"bigdata\")]/text()')\n",
    "print(html_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7800ca30",
   "metadata": {},
   "source": [
    "## 2.3.3 Scrapy爬虫实例"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3a2de",
   "metadata": {},
   "source": [
    "访问豆瓣小说 https://book.douban.com/tag/%E5%B0%8F%E8%AF%B4?start=0&type=T   使用Scrapy框架编写爬虫程序，爬取每个名句及其完整古诗内容，并把爬取到的数据分别保存到文本文件。本实例需要使用开发工具PyCharm (Community Edition)，请到PyCharm官网（https://www.jetbrains.com/pycharm/download/） 下载PyCharm安装文件并安装。\n",
    "本实例包括以下几个步骤：\n",
    "* 新建工程；\n",
    "* 编写代码文件items.py；\n",
    "* 编写爬虫文件；\n",
    "* 编写代码文件pipelines.py；\n",
    "* 编写代码文件settings.py；\n",
    "* 运行程序；\n",
    "把数据保存到csv文件中。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8a35ca",
   "metadata": {},
   "source": [
    "## 1.新建工程 \n",
    "在PyCharm中新建一个名称为“douban”的工程。在“douban”工程底部打开Terminal窗口（如图所示），在命令提示符后面输入命令“pip install scrapy”，下载Scrapy框架所需文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bb4fd4",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_1_2.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3ea97",
   "metadata": {},
   "source": [
    "下载完成后，继续输入命令“scrapy startproject douban”，创建Scrapy爬虫框架相关目录和文件。创建完成以后的具体目录结构如图所示，这些目录和文件都是由Scrapy框架自动创建的，不需要手动创建。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf94fb1",
   "metadata": {},
   "source": [
    "<img src=\"image/Chapter2_1_3.png\" width=\"600\" height=\"600\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44308dac",
   "metadata": {},
   "source": [
    "在Scrapy爬虫程序目录结构中，各个目录和文件的作用如下：  \n",
    "* Spiders目录：该目录下包含爬虫文件，需编码实现爬虫过程；  \n",
    "* __init__.py：为Python模块初始化目录，可以什么都不写，但是必须要有；  \n",
    "* items.py：模型文件，存放了需要爬取的字段；  \n",
    "* middlewares.py：中间件（爬虫中间件、下载中间件），本案例中不用此文件；  \n",
    "* pipelines.py：管道文件，用于配置数据持久化，例如写入数据库；  \n",
    "* settings.py：爬虫配置文件；  \n",
    "scrapy.cfg：项目基础设置文件，设置爬虫启用功能等。在本案例中不用此文件。  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638d6fa7",
   "metadata": {},
   "source": [
    "## 2.编写代码文件items.py   \n",
    "在items.py中定义字段用于保存数据，items.py的具体代码内容如下：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef8defe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class DoubanItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    price = scrapy.Field()\n",
    "    star = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddf3b3a",
   "metadata": {},
   "source": [
    "## 3.编写爬虫文件\n",
    "在Terminal窗口输入命令“cd douban”，进入对应的爬虫工程中，再输入命令“scrapy genspider doubanspider douban.com”，这时，在spiders目录下会出现一个新的Python文件doubanspider.py，该文件就是我们要编写爬虫程序的位置。下面是doubanspider.py的具体代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef4d993",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "from douban.items import DoubanItem\n",
    "\n",
    "class DoubanspiderSpider(scrapy.Spider):\n",
    "    # name = \"doubanspider\"\n",
    "    # allowed_domains = [\"douban.com\"]\n",
    "    # start_urls = [\"http://douban.com/\"]\n",
    "    #\n",
    "    # def parse(self, response):\n",
    "    #     pass\n",
    "\n",
    "    name = \"doubanspider\"\n",
    "    allowed_domains = [\"book.douban.com\"]\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            'https://book.douban.com/tag/%E5%B0%8F%E8%AF%B4?start=0&type=T'\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        info_list = response.xpath(\"//div//ul[@class='subject-list']/li/div[@class='info']\")\n",
    "        for info in info_list:\n",
    "            title = info.xpath(\"./h2/a/text()\").extract_first()\n",
    "            title = title.strip()\n",
    "            price = info.xpath(\"./div[@class='pub']/text()\").extract_first()\n",
    "            price = str(price).strip().split('/')[-1]\n",
    "            price = price.strip()\n",
    "            star = info.xpath(\"./div[contains(@class,'star')]/span[2]/text()\").extract_first()\n",
    "            print(title, '/', price, '/', star)\n",
    "            item = DoubanItem()\n",
    "            item['title'] = title\n",
    "            item['price'] = price\n",
    "            item['star'] = star\n",
    "            yield item\n",
    "\n",
    "        next_page = response.xpath(\"//span[contains(@class,'next')]/a/@href\").extract_first()\n",
    "        if next_page:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            print(\"--\"*20)\n",
    "            print(next_page)\n",
    "            yield scrapy.Request(url=next_page, callback=self.parse)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da26bee",
   "metadata": {},
   "source": [
    "在上面的代码中，response.xpath()返回的是scrapy.selector.unified.SelectorList对象"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98f4b72",
   "metadata": {},
   "source": [
    "## 4.编写代码文件pipelines.py  \n",
    "当我们成功获取需要的信息后，要对信息进行存储。在Scrapy爬虫框架中，当item被爬虫收集完后，将会被传递到pipelines。现在要将爬取到的数据保存到文本文件中，可以使用的pipelines.py代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cfa08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubanPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        return item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbdc81a",
   "metadata": {},
   "source": [
    "## 5.编写代码文件settings.py\n",
    "settings.py的具体代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c97075",
   "metadata": {},
   "outputs": [],
   "source": [
    "USER_AGENT = 'Mozilla/5.0 (Macintosh; U; Intel Mac OS X 10_6_8; en-us) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50'\n",
    "# Obey robots.txt rules\n",
    "# ROBOTSTXT_OBEY = True\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "\n",
    "\n",
    "COOKIES_ENABLES = False\n",
    "HTTPERROR_ALLOWED_CODES = [403]\n",
    "COMPRESSION_ENABLED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495cf1f7",
   "metadata": {},
   "source": [
    "## 6.运行程序\n",
    "有两种执行Scrapy爬虫的方法，第一种是在Terminal窗口中输入命令“scrapy crawl  doubanspider -o basic.csv”，然后回车运行，等待几秒钟后即可完成数据的爬取。第二种是在poemScrapy目录下新建Python文件run.py（run.py应与scrapy.cfg文件在同一层目录下），并输入下面代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdcddad",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrapy crawl  doubanspider -o basic.csv  -s CLOSESPIDER_ITEMCOUNT=10  \n",
    "scrapy crawl  doubanspider -o basic.json  -s CLOSESPIDER_ITEMCOUNT=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3162ed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import cmdline\n",
    "cmdline.execute(\"scrapy crawl  doubanspider -o basic.csv  -s CLOSESPIDER_ITEMCOUNT=10\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f37c40",
   "metadata": {},
   "source": [
    "# 2.4 实战体验"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8b5011",
   "metadata": {},
   "source": [
    "获取目标数据为小说名称、价格、星级三类数据。  \n",
    "从获取的数据解决一下问题：  \n",
    "（1）计算出所有爬取小说的平均星级。  \n",
    "（2）计算所有获取小说的均价。  \n",
    "打开豆瓣小说页面网址“https://book.douban.com/tag/小说?start=0&type=T”   \n",
    "用urllib和BeautifulSoap库  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d346c2b",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_1_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff670f38",
   "metadata": {},
   "source": [
    "![avatar](image/Chapter2_1_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3fbb19",
   "metadata": {},
   "source": [
    "## 1.获取网页数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df04f911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf-8\n",
    "##############################\n",
    "#Created on 2024-03-19 10:46:15\n",
    "#爬取小说\n",
    "#author: linyongyi\n",
    "#id: 20240807301\n",
    "##############################\n",
    "import requests\n",
    "from bs4 import BeautifulSoup      #导入BeautifulSoup\n",
    "data_all =[]\n",
    "\n",
    "header={'User-Agent':'Mozilla/5.0(Windows NT 6.1; Win64; x64) AppleWebKit/537.36(KHTML, like Gecko)Chrome/79.0.3945.88 Safari/537.36'}\n",
    "# for i in range(0,7660,20):\n",
    "for i in range(0,160,20):\n",
    "    url = 'https://book.douban.com/tag/小说?start=0&type=T'\n",
    "    douban_data = requests.get(url,headers=header)\n",
    "    soup = BeautifulSoup(douban_data.text,'lxml')\n",
    "    titles = soup.select('h2 a[title]') \n",
    "     #获取h2标签下a标签的title内容，即小说名称\n",
    "    prices = soup.select('div.pub')   #获取小说价格\n",
    "    stars  = soup.select('div span.rating_nums')#获取小说星级\n",
    "    for title,price,star in zip(titles,prices,stars):\n",
    "        data = {'title':title.get_text().strip().split()[0],\n",
    "                'price':price.get_text().strip().split('/')[-1],\n",
    "                'star' :star.get_text()}\n",
    "        data_all.append(data)\n",
    "#         print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d703263a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'},\n",
       " {'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'},\n",
       " {'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'},\n",
       " {'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'},\n",
       " {'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'},\n",
       " {'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'},\n",
       " {'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'},\n",
       " {'title': '太白金星有点烦', 'price': ' 45.00', 'star': '9.0'},\n",
       " {'title': '长安的荔枝', 'price': ' 45.00元', 'star': '8.5'},\n",
       " {'title': '繁花', 'price': ' 48.00元', 'star': '8.6'},\n",
       " {'title': '额尔古纳河右岸', 'price': ' 32.00元', 'star': '9.1'},\n",
       " {'title': '明亮的夜晚', 'price': ' 52', 'star': '9.0'},\n",
       " {'title': '名侦探的献祭', 'price': ' 59.00元', 'star': '7.1'},\n",
       " {'title': '活着', 'price': ' 20.00元', 'star': '9.4'},\n",
       " {'title': '疼痛部', 'price': ' 54.00元', 'star': '8.8'},\n",
       " {'title': '六个说谎的大学生', 'price': ' 45.00元', 'star': '7.7'},\n",
       " {'title': '豆子芝麻茶', 'price': ' 39.8', 'star': '8.6'},\n",
       " {'title': '秋园', 'price': ' 38.00元', 'star': '9.0'},\n",
       " {'title': '巴别塔', 'price': ' 98.00元', 'star': '8.2'},\n",
       " {'title': '一句顶一万句', 'price': ' 29.80', 'star': '8.8'},\n",
       " {'title': '绝叫', 'price': ' 58.00元', 'star': '8.9'},\n",
       " {'title': '悉达多', 'price': ' 32.00元', 'star': '9.3'},\n",
       " {'title': '生死疲劳', 'price': ' 69.90', 'star': '9.1'},\n",
       " {'title': '夜晚的潜水艇', 'price': ' 52.00元', 'star': '8.3'},\n",
       " {'title': '一句顶一万句', 'price': ' 68.00元', 'star': '9.0'},\n",
       " {'title': '我在北京送快递', 'price': ' 56.00元', 'star': '8.2'},\n",
       " {'title': '涅朵奇卡', 'price': ' 45.00元', 'star': '8.6'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05deb42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_all),'\\n',data_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd06774",
   "metadata": {},
   "source": [
    "## 2.保存数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99adb1e6",
   "metadata": {},
   "source": [
    "将已经爬取到的数据保存到file\\db_data.txt里备用。如果从网上获取不到数据（获取不到数据的原因比较多，有网页改版的可能，也有由于获取频率较高被封号的可能），请到本书提供的数据资源里下载db_data.txt，以备后用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a681218",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'file\\db_data.txt','w',encoding='utf-8') as f:\n",
    "     f.write(str(data_all))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
